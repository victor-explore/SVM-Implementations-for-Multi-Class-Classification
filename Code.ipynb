{"cells":[{"cell_type":"markdown","metadata":{"id":"OWu3_02iK7mx"},"source":["## **3.2.1 - SVM without slack formulation - binary class**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3192,"status":"ok","timestamp":1711384104299,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"Si1RIQqQryWm","outputId":"d46621f3-83b1-41ec-c26d-6100f3799169"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Step 1: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":618,"status":"ok","timestamp":1711384107124,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"qmWHqeVyn_3g","outputId":"43c378e2-1b93-4a09-994c-b9acb7f779df"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape: (11200, 10)\n","y_train shape: (11200,)\n","X_test shape: (2800, 10)\n","y_test shape: (2800,)\n"]}],"source":["# Step 2: Load the Dataset\n","import numpy as np\n","\n","file_path = '/content/drive/MyDrive/PRNN Assignment 2/multi_class_classification_data_group_25_train.txt'\n","\n","# Assuming the first row is the header and the last column is the label\n","data = np.loadtxt(file_path, delimiter='\\t', skiprows=1)\n","\n","# Step 3: Split the Data into features and labels\n","X = data[:, :-1]  # All rows, all columns except the last one\n","y = data[:, -1]   # All rows, last column only\n","\n","# Step 4: Split the Data into Training and Testing sets\n","# Decide on a split ratio, for example, 80% training and 20% testing\n","split_ratio = 0.8\n","split_index = int(X.shape[0] * split_ratio)\n","\n","# Randomly shuffle the data\n","indices = np.arange(X.shape[0])\n","np.random.shuffle(indices)\n","X = X[indices]\n","y = y[indices]\n","\n","# Split the data\n","X_train, X_test = X[:split_index], X[split_index:]\n","y_train, y_test = y[:split_index], y[split_index:]\n","\n","print(f\"X_train shape: {X_train.shape}\")\n","print(f\"y_train shape: {y_train.shape}\")\n","print(f\"X_test shape: {X_test.shape}\")\n","print(f\"y_test shape: {y_test.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711384111084,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"3Zmhh0H4tPCc","outputId":"6af5a82f-03d0-4e67-8d5e-8cf37b86aced"},"outputs":[{"name":"stdout","output_type":"stream","text":["Transformed labels (train): [-1.  1.]\n","Transformed labels (test): [-1.  1.]\n"]}],"source":["y_train_transformed = y_train * 2 - 1  # This transforms 0 to -1 and 1 to 1\n","y_test_transformed = y_test * 2 - 1  # Same transformation for the test set\n","\n","# Debug print statements to check if the transformation is correct\n","print(\"Transformed labels (train):\", np.unique(y_train_transformed))\n","print(\"Transformed labels (test):\", np.unique(y_test_transformed))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":986,"status":"ok","timestamp":1711384114959,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"UGcOtJzs1Uvr","outputId":"bdc92827-2eca-4d00-fec5-9bad75dd3458"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal weight vector (w): [ 1.33333333 -0.66666667]\n","Optimal bias (b): -0.3333333363967996\n","Solver status: optimal\n"]}],"source":["import cvxpy as cp\n","import numpy as np\n","\n","# Simple and clearly linearly separable dataset\n","X = np.array([[2, 2], [4, 4], [0, 1], [1, 3]])\n","y = np.array([1, 1, -1, -1])\n","\n","# Variables for the optimization problem\n","n_samples, n_features = X.shape\n","w = cp.Variable(n_features)\n","b = cp.Variable()\n","\n","# Objective function: 1/2 * ||w||^2\n","objective = cp.Minimize(0.5 * cp.norm(w, 2)**2)\n","\n","# Constraints: y_i * (w^T * x_i + b) >= 1 for all samples\n","constraints = [y[i] * (X[i] @ w + b) >= 1 for i in range(n_samples)]\n","\n","# Solve the problem\n","prob = cp.Problem(objective, constraints)\n","prob.solve()\n","\n","# Results\n","w_opt = w.value\n","b_opt = b.value\n","\n","print(f\"Optimal weight vector (w): {w_opt}\")\n","print(f\"Optimal bias (b): {b_opt}\")\n","print(\"Solver status:\", prob.status)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":471,"status":"ok","timestamp":1711384119187,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"4A2lOies68nD","outputId":"10a41e6f-ca92-4448-81d1-7f59f2aa1e9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal weight vector (w) for the 100-point subset: None\n","Optimal bias (b) for the 100-point subset: None\n","Solver status for the 100-point subset: infeasible\n"]}],"source":["import cvxpy as cp\n","import numpy as np\n","\n","# Select the first 100 data points\n","X_subset = X_train[:100]\n","y_subset = y_train_transformed[:100]\n","\n","# Number of samples and features in the subset\n","n_samples_subset, n_features_subset = X_subset.shape\n","\n","# Optimization variables\n","w = cp.Variable(n_features_subset)\n","b = cp.Variable()\n","\n","# Objective function\n","objective = cp.Minimize(0.5 * cp.norm(w, 2)**2)\n","\n","# Constraints\n","constraints = [y_subset[i] * (X_subset[i] @ w + b) >= 1 for i in range(n_samples_subset)]\n","\n","# Solve the problem\n","prob = cp.Problem(objective, constraints)\n","prob.solve()\n","\n","# Results\n","w_opt = w.value\n","b_opt = b.value\n","\n","print(f\"Optimal weight vector (w) for the 100-point subset: {w_opt}\")\n","print(f\"Optimal bias (b) for the 100-point subset: {b_opt}\")\n","print(\"Solver status for the 100-point subset:\", prob.status)"]},{"cell_type":"markdown","metadata":{"id":"x6emrp3YKSML"},"source":["## **3.2.2 - SVM with slack formulation - binary class**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70520,"status":"ok","timestamp":1711383115214,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"Rm1MjW2q8LKp","outputId":"6e534e40-c960-4842-df29-f8f45c31a632"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal weight vector (w) for the full dataset: [-0.16792165  0.31727058 -0.54082606 -0.1616919   0.14615286  0.27779174\n","  0.3074921  -0.5230517  -0.68889371 -0.30658345]\n","Optimal bias (b) for the full dataset: 0.6057291430156447\n","Solver status for the full dataset: optimal\n"]}],"source":["import cvxpy as cp\n","import numpy as np\n","\n","# Assuming X_train and y_train_transformed are your full dataset and labels\n","n_samples, n_features = X_train.shape\n","\n","# Variables for optimization\n","w = cp.Variable(n_features)\n","b = cp.Variable()\n","xi = cp.Variable(n_samples)  # Slack variables for the full dataset\n","\n","# Regularization parameter\n","C = .1\n","\n","# Objective function for the full dataset\n","objective = cp.Minimize(0.5 * cp.norm(w, 2)**2 + C * cp.sum(xi))\n","\n","# Constraints for the full dataset\n","constraints = [y_train_transformed[i] * (X_train[i] @ w + b) >= 1 - xi[i] for i in range(n_samples)] + [xi[i] >= 0 for i in range(n_samples)]\n","\n","# Solve the problem\n","prob = cp.Problem(objective, constraints)\n","prob.solve()\n","\n","# Results for the full dataset\n","w_opt_full = w.value\n","b_opt_full = b.value\n","\n","print(f\"Optimal weight vector (w) for the full dataset: {w_opt_full}\")\n","print(f\"Optimal bias (b) for the full dataset: {b_opt_full}\")\n","print(\"Solver status for the full dataset:\", prob.status)"]},{"cell_type":"markdown","metadata":{"id":"6xeFrat08yOp"},"source":["**Test**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711383140723,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"USve75gP8zuu","outputId":"e8f98fd3-40f5-495d-aad5-326b5e9a594c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model accuracy on the test dataset: 34.14%\n"]}],"source":["n_test_samples, _ = X_test.shape\n","\n","# Calculate the decision function for each test sample\n","decision_function = X_test.dot(w_opt_full) + b_opt_full\n","\n","# Make predictions based on the sign of the decision function\n","predictions = np.sign(decision_function)\n","\n","# y_test_transformed are true labels for the test dataset\n","# Convert predictions from -1,1 to 0,1 if original labels were 0 and 1\n","predictions_transformed = (predictions + 1) // 2\n","\n","# Calculate accuracy\n","accuracy = np.mean(predictions_transformed == y_test_transformed)\n","print(f\"Model accuracy on the test dataset: {accuracy * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":541,"status":"ok","timestamp":1711383144380,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"p0pZGN0_XX-v","outputId":"ea94d9c0-9359-4b75-cef4-9efcca60f60e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model accuracy on the training dataset: 33.66%\n"]}],"source":["# Calculate the decision function for each training sample\n","decision_function_train = X_train.dot(w_opt_full) + b_opt_full\n","\n","# Make predictions based on the sign of the decision function\n","predictions_train = np.sign(decision_function_train)\n","\n","# y_train_transformed are true labels for the training dataset\n","# Convert predictions from -1,1 to 0,1 if original labels were 0 and 1\n","predictions_train_transformed = (predictions_train + 1) // 2\n","\n","# Calculate accuracy on training data\n","accuracy_train = np.mean(predictions_train_transformed == y_train_transformed)\n","print(f\"Model accuracy on the training dataset: {accuracy_train * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"LX9vg0vYZRDl"},"source":["**Linear kernel**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":374,"status":"ok","timestamp":1711385482148,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"3rtCpBfFheyD","outputId":"65c8fbbb-d84c-4ed5-c2df-da50e2887e75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal weight vector (w): [ 0.30952046  0.55017813 -0.2130002  -0.49864981  0.54300217  0.25091342\n"," -0.54073784 -0.62652359 -0.76701829 -0.04458616]\n","Optimal bias (b): 0.18462387572828234\n"]}],"source":["import numpy as np\n","from scipy.optimize import minimize\n","\n","n_features = X_subset.shape[1]\n","\n","# Objective function for the primal problem\n","def svm_primal_objective(params):\n","    w = params[:-1]\n","    b = params[-1]\n","    regularization = 0.5 * np.sum(w ** 2)\n","    hinge_loss = np.sum(np.maximum(0, 1 - y_subset * (X_subset.dot(w) + b)))\n","    return regularization + C * hinge_loss\n","\n","# Regularization parameter C\n","C = 1\n","\n","# Initial guess for w and b\n","initial_params = np.zeros(n_features + 1)\n","\n","# Optimization\n","result = minimize(svm_primal_objective, initial_params, method='SLSQP')\n","\n","if result.success:\n","    optimized_params = result.x\n","    w_opt = optimized_params[:-1]\n","    b_opt = optimized_params[-1]\n","    print(f\"Optimal weight vector (w): {w_opt}\")\n","    print(f\"Optimal bias (b): {b_opt}\")\n","else:\n","    print(\"Optimization was not successful. Check the objective function and constraints.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":451,"status":"ok","timestamp":1711386034593,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"32rU4tVqXYUs","outputId":"1ef065c4-4dbf-4871-903c-2b31846e32cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model accuracy on the test dataset: 62.04%\n"]}],"source":["\n","# Calculate the decision values for each test sample\n","decision_values = X_test.dot(w_opt) + b_opt\n","\n","# Predict class labels based on the sign of the decision values\n","predictions = np.sign(decision_values)\n","\n","# Convert predictions from -1,1 to the original label space if necessary\n","\n","# Calculate accuracy\n","accuracy = np.mean(predictions == y_test_transformed)\n","print(f\"Model accuracy on the test dataset: {accuracy * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"-ey63qh9kDAj"},"source":["**Polynomial kernel**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yK_HUuCskGMH"},"outputs":[],"source":["def polynomial_kernel(X, Y=None, degree=3, gamma=None, coef0=1):\n","    if Y is None:\n","        Y = X\n","    if gamma is None:\n","        gamma = 1.0 / X.shape[1]  # Default value of gamma is 1 / n_features\n","\n","    K = (gamma * np.dot(X, Y.T) + coef0) ** degree\n","    return K"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1711386393752,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"SeZhlXQOkqrq","outputId":"990a6230-343b-480c-ff31-5c68221c1e66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimization successful.\n"]}],"source":["import cvxpy as cp\n","import numpy as np\n","\n","# Define polynomial kernel\n","def polynomial_kernel(X, Y=None, degree=3, gamma=None, coef0=1):\n","    if Y is None:\n","        Y = X\n","    if gamma is None:\n","        gamma = 1.0 / X.shape[1]\n","    K = (gamma * np.dot(X, Y.T) + coef0) ** degree\n","    return K\n","\n","degree = 3\n","gamma = None  # Automatically set to 1/n_features within the function\n","coef0 = 1\n","\n","# Compute the kernel matrix\n","K = polynomial_kernel(X_subset, degree=degree, gamma=gamma, coef0=coef0)\n","\n","# Symmetrize the matrix used in quadratic form\n","Q = np.multiply(np.outer(y_subset, y_subset), K)\n","Q_sym = 0.5 * (Q + Q.T)  # Symmetrizing the matrix\n","\n","n_samples = X_subset.shape[0]\n","alpha = cp.Variable(n_samples)\n","\n","# Dual problem objective with corrected matrix\n","objective = cp.Maximize(\n","    cp.sum(alpha) - 0.5 * cp.quad_form(alpha, Q_sym)\n",")\n","\n","constraints = [\n","    alpha >= 0,\n","    alpha <= 1,  # Assuming C=1; adjust as needed\n","    cp.sum(cp.multiply(alpha, y_subset)) == 0\n","]\n","\n","# Solve the problem\n","prob = cp.Problem(objective, constraints)\n","result = prob.solve()\n","\n","# Check if the optimization was successful\n","if result is not None:\n","    optimized_alphas = alpha.value\n","    print(\"Optimization successful.\")\n","else:\n","    print(\"Optimization failed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":368,"status":"ok","timestamp":1711386581155,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"XUodQ-24lI5T","outputId":"eecb67fc-c4d9-4786-8017-b535c2839314"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal bias (b): 0.37653078023302344\n"]}],"source":["# Indices of support vectors\n","sv_indices = optimized_alphas > 1e-5\n","\n","# Alphas for support vectors\n","alphas_sv = optimized_alphas[sv_indices]\n","\n","# Support vectors and their labels\n","X_sv = X_subset[sv_indices]\n","y_sv = y_subset[sv_indices]\n","\n","# Calculate bias using support vectors\n","b = np.mean(\n","    [y_k - np.sum(alphas_sv * y_sv * polynomial_kernel(X_sv, x_k[np.newaxis, :], degree=degree, gamma=gamma, coef0=coef0).flatten())\n","     for y_k, x_k in zip(y_sv, X_sv)]\n",")\n","\n","print(f\"Optimal bias (b): {b}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzO9KhAjlyma"},"outputs":[],"source":["def predict(X_new, X_sv, y_sv, alphas_sv, b, kernel):\n","    decision_function = np.sum(alphas_sv * y_sv * kernel(X_sv, X_new), axis=0) + b\n","    return np.sign(decision_function)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":342,"status":"ok","timestamp":1711386660833,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"dtur1qYSl4c7","outputId":"e5713c14-1403-4fcc-bfd8-9ab8a8be0e8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model accuracy on the test dataset: 66.64%\n"]}],"source":["def predict(X_new, X_sv, y_sv, alphas_sv, b, kernel, degree=3, gamma=None, coef0=1):\n","    \"\"\"\n","    Predict the class labels for new data points with the trained SVM model.\n","\n","    :param X_new: New data points to predict\n","    :param X_sv: Support vectors from the training data\n","    :param y_sv: Labels of the support vectors\n","    :param alphas_sv: Optimized alpha values for the support vectors\n","    :param b: Calculated bias of the SVM model\n","    :param kernel: Kernel function used by the SVM model\n","    :param degree: Degree for the polynomial kernel\n","    :param gamma: Scale parameter for the polynomial kernel\n","    :param coef0: Independent term in polynomial kernel\n","    :return: Predicted class labels for X_new\n","    \"\"\"\n","    K = kernel(X_sv, X_new, degree=degree, gamma=gamma, coef0=coef0)\n","    decision_values = np.dot(alphas_sv * y_sv, K) + b\n","    return np.sign(decision_values)\n","\n","predictions = predict(X_test, X_sv, y_sv, alphas_sv, b, polynomial_kernel, degree=degree, gamma=gamma, coef0=coef0)\n","\n","# Calculate accuracy\n","accuracy = np.mean(predictions == y_test_transformed)\n","print(f\"Model accuracy on the test dataset: {accuracy * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"3JAzMLSsLMMr"},"source":["## **3.2.3 - SVM without slack formulation - multiclass**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4454,"status":"ok","timestamp":1711558501951,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"WhmWs5a8IOcg","outputId":"885d8f03-efd1-4574-e732-8e31e10127e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2990,"status":"ok","timestamp":1711558506231,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"cuW4w02sJqBO","outputId":"820d989f-865c-4b79-ae00-8675e9b80a44"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data loaded successfully\n"]}],"source":["import numpy as np\n","\n","file_path = '/content/drive/MyDrive/PRNN Assignment 2/multi_class_classification_data_group_25_train2.txt'\n","\n","# Skipping the first row since it contains headers.\n","data = np.genfromtxt(file_path, delimiter='\\t', skip_header=1)\n","\n","print(\"Data loaded successfully\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711558507801,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"UsIWps-PJ4y5","outputId":"4f77baf9-ec4e-4fa4-e96a-006a0dc30d00"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training data shape: (56000, 26)\n","Testing data shape: (14000, 26)\n"]}],"source":["# Ensure reproducibility\n","np.random.seed(42)\n","\n","# Shuffle the data\n","np.random.shuffle(data)\n","\n","# Define the split ratio\n","split_ratio = 0.8\n","\n","# Calculate the split index\n","split_index = int(data.shape[0] * split_ratio)\n","\n","# Split the data\n","train_data = data[:split_index]\n","test_data = data[split_index:]\n","\n","print(f\"Training data shape: {train_data.shape}\")\n","print(f\"Testing data shape: {test_data.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":431,"status":"ok","timestamp":1711558510611,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"UftLPrfNLz_L","outputId":"56f975fe-252f-4bd3-cae0-a33d3a7e07bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Before normalization:\n","Features shape: (56000, 25)\n","First 5 rows of features:\n"," [[ 0.88231309  2.060794   -0.03727296  0.3633859   0.712504   -1.54848548\n","   0.74139339  0.50145251  0.82316467  0.50114331  0.30472922  2.93303774\n","   0.11544043  0.57320856  0.30664962 -1.35473103  0.01571383 -0.37885504\n","   0.90970749  0.76453455  0.88939557  0.08771643  0.46922044  1.98547878\n","   0.9951316 ]\n"," [-0.60835611 -0.42902146 -0.02019832 -1.19107419  0.35161801  0.02221423\n","   1.00612765  2.57896898 -0.21904903 -0.08734955 -0.88490681  1.4437579\n","   0.3738467   0.19311729  0.35989156  1.22988802  1.67289023  0.54611338\n","   0.13648412  0.89357358  0.16569693 -0.37054891 -0.34939985  0.61531351\n","   2.05849387]\n"," [ 0.19217755  0.62725995 -0.37947646  1.77115989  0.23299918 -0.08514341\n","   0.3335442   0.32321727  0.88810755  0.60299201  0.53586898 -0.44658212\n","   0.37783549  0.30922605  1.3429528   0.0675     -0.46004075  0.38989001\n","  -0.01722178 -0.09530088 -0.86920728 -0.21681016  1.69519322  0.70981577\n","   0.70220039]\n"," [ 1.48864121 -0.16311634  0.92171693  1.37181084  1.16485888  0.2705424\n","   1.55712357  0.88107639  0.01439043  1.58838119 -0.17825917  1.01691382\n","   1.44293307  0.70848596  0.42565033  0.09387264  0.52512463  0.76205504\n","  -0.40278952 -0.82093124  1.86442885  0.59077897  0.3583779   0.24932479\n","   0.29038044]\n"," [ 0.89131569 -0.3516962   1.66659477  0.31194421  0.82382267  0.96902403\n","   0.88431504  0.560182    0.1736649  -1.06336648  0.05622122  0.05458604\n","   1.12472879  0.47454637  0.25924215  0.26695797  0.53404091  2.01665223\n","  -0.15353312 -0.03313636 -0.03610433  0.10063598  1.00251878  1.44823933\n","  -0.97381136]]\n","Labels shape: (56000,)\n","First 5 labels: [7. 9. 6. 8. 5.]\n","\n","After normalization:\n","Normalized features shape: (56000, 25)\n","First 5 rows of normalized features:\n"," [[ 4.60257220e-01  2.10700909e+00 -7.14428913e-01 -1.04552769e-01\n","   2.22346549e-01 -2.72329302e+00  3.12506312e-01  2.69758110e-02\n","   4.70898394e-01 -6.71009917e-03 -2.75580530e-01  3.22021225e+00\n","  -4.54861006e-01  1.34388623e-01 -3.04395153e-01 -2.59630409e+00\n","  -6.96113553e-01 -1.03421641e+00  5.48603427e-01  3.24392102e-01\n","   5.45035038e-01 -4.75087364e-01  1.34788933e-04  1.88409531e+00\n","   5.75189489e-01]\n"," [-1.52403791e+00 -1.23031669e+00 -6.92242464e-01 -2.31237868e+00\n","  -2.60856163e-01 -6.25016336e-01  6.41045522e-01  2.67341227e+00\n","  -9.36633297e-01 -7.71871606e-01 -1.80412134e+00  1.24264284e+00\n","  -1.11122841e-01 -3.41416656e-01 -2.32360348e-01  1.04900210e+00\n","   1.55419783e+00  1.25752719e-01 -4.81678635e-01  5.02001731e-01\n","  -4.19206340e-01 -1.04588616e+00 -1.05416620e+00  1.66227855e-01\n","   1.96740902e+00]\n"," [-4.58412452e-01  1.85513216e-01 -1.15908131e+00  1.89493240e+00\n","  -4.19679009e-01 -7.68433963e-01 -1.93640676e-01 -2.00068468e-01\n","   5.58605129e-01  1.25714116e-01  2.14065777e-02 -1.26748217e+00\n","  -1.05816857e-01 -1.96069560e-01  1.09769313e+00 -5.90411748e-01\n","  -1.34214975e+00 -7.01613094e-02 -6.86484188e-01 -8.59087414e-01\n","  -1.79809170e+00 -8.54394712e-01  1.57906502e+00  2.84711632e-01\n","   1.91665856e-01]\n"," [ 1.26736717e+00 -8.73899905e-01  5.31663406e-01  1.32773020e+00\n","   8.28020124e-01 -2.93278056e-01  1.32483977e+00  5.10558253e-01\n","  -6.21368354e-01  1.40692229e+00 -8.96163168e-01  6.75849568e-01\n","   1.31100124e+00  3.03731374e-01 -1.43390633e-01 -5.53216177e-01\n","  -4.37492413e-03  3.96557275e-01 -1.20023421e+00 -1.85784675e+00\n","   1.84414959e+00  1.51509200e-01 -1.42619305e-01 -2.92636578e-01\n","  -3.47514241e-01]\n"," [ 4.72240976e-01 -1.12667062e+00  1.49954280e+00 -1.77616276e-01\n","   3.71394966e-01  6.39814180e-01  4.89874242e-01  1.01788152e-01\n","  -4.06264810e-01 -2.04089393e+00 -5.94883747e-01 -6.01996217e-01\n","   8.87718342e-01  1.08814683e-02 -3.68536100e-01 -3.09099343e-01\n","   7.73266476e-03  1.96990180e+00 -8.68112332e-01 -7.73524019e-01\n","  -6.88082235e-01 -4.58995240e-01  6.86969675e-01  1.21052242e+00\n","  -2.00267221e+00]]\n","\n","Mean of normalized features (should be close to 0):\n"," [2.23132623e-15 5.05558492e-15 1.26696273e-16 1.44609820e-15\n"," 7.66097503e-16]\n","Standard deviation of normalized features (should be close to 1):\n"," [1. 1. 1. 1. 1.]\n"]}],"source":["import numpy as np\n","\n","# Extracting features and labels\n","features = train_data[:, :-1]  # Exclude the last column, which is the label\n","labels = train_data[:, -1]  # The last column is the label\n","\n","# Debug print before normalization\n","print(\"Before normalization:\")\n","print(\"Features shape:\", features.shape)\n","print(\"First 5 rows of features:\\n\", features[:5, :])\n","print(\"Labels shape:\", labels.shape)\n","print(\"First 5 labels:\", labels[:5])\n","\n","# Normalize features\n","mean = np.mean(features, axis=0)\n","std = np.std(features, axis=0)\n","normalized_features = (features - mean) / std\n","\n","# Debug print after normalization\n","print(\"\\nAfter normalization:\")\n","print(\"Normalized features shape:\", normalized_features.shape)\n","print(\"First 5 rows of normalized features:\\n\", normalized_features[:5, :])\n","\n","# Confirming mean ~ 0 and std ~ 1\n","print(\"\\nMean of normalized features (should be close to 0):\\n\", np.mean(normalized_features, axis=0)[:5])\n","print(\"Standard deviation of normalized features (should be close to 1):\\n\", np.std(normalized_features, axis=0)[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":353,"status":"ok","timestamp":1711558516028,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"-9kOVINdMIj_","outputId":"6d97f62e-3c81-40ce-e63a-58edc8658657"},"outputs":[{"name":"stdout","output_type":"stream","text":["First 20 labels (original): [7. 9. 6. 8. 5. 2. 8. 6. 1. 0. 1. 6. 3. 7. 5. 3. 1. 8. 3. 2.]\n","First 20 binary labels for class 0: [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n"]}],"source":["def prepare_binary_labels(labels, class_label):\n","    \"\"\"\n","    This function prepares binary labels for one-vs-rest classification.\n","    All instances of the specified class_label are marked as 1, and all other instances are marked as 0.\n","\n","    :param labels: The array of multi-class labels.\n","    :param class_label: The current class for which the binary labels are being prepared.\n","    :return: An array of binary labels.\n","    \"\"\"\n","    binary_labels = np.where(labels == class_label, 1, 0)\n","    return binary_labels\n","\n","# Example usage for class 0\n","class_0_labels = prepare_binary_labels(labels, 0)\n","\n","# Debug print to verify binary labels for class 0\n","print(\"First 20 labels (original):\", labels[:20])\n","print(\"First 20 binary labels for class 0:\", class_0_labels[:20])"]},{"cell_type":"markdown","metadata":{"id":"Lh5kl3CfOB5b"},"source":["To focus on computational efficiency and demonstrate the concept with limited resources, we'll adjust the script to train SVM classifiers for each class using only the first 100 data points from the training dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6583,"status":"ok","timestamp":1711548183124,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"N_FaCjgEOCvM","outputId":"19d0a316-18c0-460d-d51f-00e3130c7f5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training SVM for class 0 vs rest\n","Finished training SVM for class 0 vs rest. Solver status: optimal\n","Training SVM for class 1 vs rest\n","Finished training SVM for class 1 vs rest. Solver status: infeasible\n","Training SVM for class 2 vs rest\n","Finished training SVM for class 2 vs rest. Solver status: optimal\n","Training SVM for class 3 vs rest\n","Finished training SVM for class 3 vs rest. Solver status: optimal\n","Training SVM for class 4 vs rest\n","Finished training SVM for class 4 vs rest. Solver status: infeasible\n","Training SVM for class 5 vs rest\n","Finished training SVM for class 5 vs rest. Solver status: optimal\n","Training SVM for class 6 vs rest\n","Finished training SVM for class 6 vs rest. Solver status: optimal\n","Training SVM for class 7 vs rest\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/cvxpy/problems/problem.py:1387: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Finished training SVM for class 7 vs rest. Solver status: optimal_inaccurate\n","Training SVM for class 8 vs rest\n","Finished training SVM for class 8 vs rest. Solver status: infeasible\n","Training SVM for class 9 vs rest\n","Finished training SVM for class 9 vs rest. Solver status: optimal\n","Training complete for all classes.\n"]}],"source":["import cvxpy as cp\n","import numpy as np\n","\n","# Limit to the first 100 data points for demonstration\n","X_train_subset = normalized_features[:100]\n","y_train_subset = labels[:100]\n","\n","num_classes = len(np.unique(y_train_subset))  # Number of unique classes\n","\n","# Initialize lists to store the optimal weights and biases for each SVM\n","optimal_weights = []\n","optimal_biases = []\n","\n","for class_label in range(num_classes):\n","    print(f\"Training SVM for class {class_label} vs rest\")\n","\n","    # Prepare binary labels for the current class vs all others\n","    y_train_transformed = np.where(y_train_subset == class_label, 1, -1)\n","\n","    # Optimization variables\n","    w = cp.Variable(X_train_subset.shape[1])\n","    b = cp.Variable()\n","\n","    # Objective function\n","    objective = cp.Minimize(0.5 * cp.norm(w, 2)**2)\n","\n","    # Constraints\n","    constraints = [y_train_transformed[i] * (X_train_subset[i] @ w + b) >= 1 for i in range(100)]\n","\n","    # Solve the problem\n","    prob = cp.Problem(objective, constraints)\n","    prob.solve()\n","\n","    # Store the results\n","    optimal_weights.append(w.value)\n","    optimal_biases.append(b.value)\n","\n","    print(f\"Finished training SVM for class {class_label} vs rest. Solver status: {prob.status}\")\n","\n","print(\"Training complete for all classes.\")"]},{"cell_type":"markdown","metadata":{"id":"nOUXQXBpLPFs"},"source":["## **3.2.4 - SVM with slack formulation - multiclass**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6912,"status":"ok","timestamp":1711548547431,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"L07EzXYQOoEs","outputId":"dfbac0d7-646c-4e91-d399-4855085978a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training SVM with slacks for class 0 vs rest\n","Finished training SVM with slacks for class 0 vs rest. Solver status: optimal\n","Training SVM with slacks for class 1 vs rest\n","Finished training SVM with slacks for class 1 vs rest. Solver status: optimal\n","Training SVM with slacks for class 2 vs rest\n","Finished training SVM with slacks for class 2 vs rest. Solver status: optimal\n","Training SVM with slacks for class 3 vs rest\n","Finished training SVM with slacks for class 3 vs rest. Solver status: optimal\n","Training SVM with slacks for class 4 vs rest\n","Finished training SVM with slacks for class 4 vs rest. Solver status: optimal\n","Training SVM with slacks for class 5 vs rest\n","Finished training SVM with slacks for class 5 vs rest. Solver status: optimal\n","Training SVM with slacks for class 6 vs rest\n","Finished training SVM with slacks for class 6 vs rest. Solver status: optimal\n","Training SVM with slacks for class 7 vs rest\n","Finished training SVM with slacks for class 7 vs rest. Solver status: optimal\n","Training SVM with slacks for class 8 vs rest\n","Finished training SVM with slacks for class 8 vs rest. Solver status: optimal\n","Training SVM with slacks for class 9 vs rest\n","Finished training SVM with slacks for class 9 vs rest. Solver status: optimal\n","Training complete for all classes with slacks.\n"]}],"source":["import cvxpy as cp\n","import numpy as np\n","\n","\n","X_train = normalized_features[:100]  # Using the first 100 data points\n","y_train = labels[:100]\n","\n","num_classes = len(np.unique(y_train))  # Number of unique classes\n","\n","# Initialize lists to store the optimal weights and biases for each SVM\n","optimal_weights = []\n","optimal_biases = []\n","\n","# Regularization parameter C\n","C = 1\n","\n","for class_label in range(num_classes):\n","    print(f\"Training SVM with slacks for class {class_label} vs rest\")\n","\n","    # Prepare binary labels for the current class vs all others\n","    y_train_transformed = np.where(y_train == class_label, 1, -1)\n","\n","    # Optimization variables\n","    w = cp.Variable(X_train.shape[1])\n","    b = cp.Variable()\n","    xi = cp.Variable(X_train.shape[0])  # Slack variable for each data point\n","\n","    # Objective function\n","    objective = cp.Minimize(0.5 * cp.norm(w, 2)**2 + C * cp.sum(xi))\n","\n","    # Constraints\n","    constraints = [y_train_transformed[i] * (X_train[i] @ w + b) >= 1 - xi[i] for i in range(X_train.shape[0])]\n","    constraints += [xi[i] >= 0 for i in range(X_train.shape[0])]  # Slack variables must be non-negative\n","\n","    # Solve the problem\n","    prob = cp.Problem(objective, constraints)\n","    prob.solve()\n","\n","    # Store the results\n","    optimal_weights.append(w.value)\n","    optimal_biases.append(b.value)\n","\n","    print(f\"Finished training SVM with slacks for class {class_label} vs rest. Solver status: {prob.status}\")\n","\n","print(\"Training complete for all classes with slacks.\")"]},{"cell_type":"markdown","metadata":{"id":"djXxBRpgP1ls"},"source":["Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1711548832989,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"cAwB4NIjP3ZC","outputId":"d588e4ac-5b32-435a-e234-ddc3b3ac3a38"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on the test data: 0.1484\n"]}],"source":["# Extracting test features and labels\n","test_features = test_data[:, :-1]  # Exclude the last column, which is the label\n","test_labels = test_data[:, -1]  # The last column is the label\n","\n","# Normalize test features using mean and std from the training features\n","normalized_test_features = (test_features - mean) / std\n","\n","# Ensure the predict function is defined as before\n","def predict(X, weights, biases):\n","    # Calculate the score for each classifier\n","    scores = np.array([X.dot(w) + b for w, b in zip(weights, biases)])\n","    # Determine the predicted class with the highest score\n","    predicted_classes = np.argmax(scores, axis=0)\n","    return predicted_classes\n","\n","# Predict labels for the normalized test features\n","predicted_labels = predict(normalized_test_features, optimal_weights, optimal_biases)\n","\n","# Calculate accuracy by comparing predicted labels with actual test labels\n","accuracy = np.mean(predicted_labels == test_labels)\n","print(f\"Accuracy on the test data: {accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"UFcv3XOrQsQB"},"source":["**Linear Kernels**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58659,"status":"ok","timestamp":1711550645680,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"hQG0SwffRHyY","outputId":"03984b81-d3e0-4115-c551-847a420e07d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training for class 0 vs rest\n","Optimization terminated successfully    (Exit mode 0)\n","            Current function value: 1121.8017948380716\n","            Iterations: 111\n","            Function evaluations: 3247\n","            Gradient evaluations: 111\n","Class 0: Optimization successful.\n","\n","Training for class 1 vs rest\n","Optimization terminated successfully    (Exit mode 0)\n","            Current function value: 1119.6013303331617\n","            Iterations: 103\n","            Function evaluations: 3031\n","            Gradient evaluations: 103\n","Class 1: Optimization successful.\n","\n","Training for class 2 vs rest\n","Optimization terminated successfully    (Exit mode 0)\n","            Current function value: 1111.8013971304365\n","            Iterations: 111\n","            Function evaluations: 3242\n","            Gradient evaluations: 111\n","Class 2: Optimization successful.\n","\n","Training for class 3 vs rest\n","Optimization terminated successfully    (Exit mode 0)\n","            Current function value: 1118.6017522874613\n","            Iterations: 108\n","            Function evaluations: 3163\n","            Gradient evaluations: 108\n","Class 3: Optimization successful.\n","\n","Training for class 4 vs rest\n","Optimization terminated successfully    (Exit mode 0)\n","            Current function value: 1114.401853747946\n","            Iterations: 109\n","            Function evaluations: 3196\n","            Gradient evaluations: 109\n","Class 4: Optimization successful.\n","\n","Training for class 5 vs rest\n","Optimization terminated successfully    (Exit mode 0)\n","            Current function value: 1124.601848543932\n","            Iterations: 108\n","            Function evaluations: 3161\n","            Gradient evaluations: 108\n","Class 5: Optimization successful.\n","\n","Training for class 6 vs rest\n","Optimization terminated successfully    (Exit mode 0)\n","            Current function value: 1134.4019315178525\n","            Iterations: 110\n","            Function evaluations: 3218\n","            Gradient evaluations: 110\n","Class 6: Optimization successful.\n","\n","Training for class 7 vs rest\n","Optimization terminated successfully    (Exit mode 0)\n","            Current function value: 1114.402210896488\n","            Iterations: 113\n","            Function evaluations: 3301\n","            Gradient evaluations: 113\n","Class 7: Optimization successful.\n","\n","Training for class 8 vs rest\n","Optimization terminated successfully    (Exit mode 0)\n","            Current function value: 1105.8017663678409\n","            Iterations: 108\n","            Function evaluations: 3152\n","            Gradient evaluations: 108\n","Class 8: Optimization successful.\n","\n","Training for class 9 vs rest\n","Optimization terminated successfully    (Exit mode 0)\n","            Current function value: 1134.6016870199373\n","            Iterations: 115\n","            Function evaluations: 3356\n","            Gradient evaluations: 115\n","Class 9: Optimization successful.\n"]}],"source":["import numpy as np\n","from scipy.optimize import minimize\n","\n","num_classes = len(np.unique(labels))\n","\n","optimal_weights = []\n","optimal_biases = []\n","\n","C = 0.1  # Regularization parameter\n","\n","for class_label in range(num_classes):\n","    print(f\"\\nTraining for class {class_label} vs rest\")\n","\n","    y_subset = np.where(labels == class_label, 1, -1)\n","    X_subset = normalized_features\n","\n","    def svm_primal_objective(params):\n","        w = params[:-1]\n","        b = params[-1]\n","        regularization = 0.5 * np.sum(w ** 2)\n","        hinge_loss = np.sum(np.maximum(0, 1 - y_subset * (X_subset.dot(w) + b)))\n","        return regularization + C * hinge_loss\n","\n","    n_features = X_subset.shape[1]\n","    initial_params = np.zeros(n_features + 1)\n","\n","    # Optimization with increased iterations limit\n","    result = minimize(svm_primal_objective, initial_params, method='SLSQP', bounds=[(None, None)] * (n_features + 1), options={'disp': True, 'maxiter': 1000})  # Increased maxiter\n","\n","    if result.success:\n","        optimized_params = result.x\n","        w_opt = optimized_params[:-1]\n","        b_opt = optimized_params[-1]\n","        optimal_weights.append(w_opt)\n","        optimal_biases.append(b_opt)\n","        print(f\"Class {class_label}: Optimization successful.\")\n","    else:\n","        print(f\"Class {class_label}: Optimization was not successful. Message: {result.message}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":566,"status":"ok","timestamp":1711557066044,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"OzDu0mIEv1JG","outputId":"62d783b5-930f-49ad-ce1d-cd2f51e8761c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on the test data: 0.1585\n"]}],"source":["\n","# Extracting test features and labels\n","test_features = test_data[:, :-1]  # Exclude the last column, which is the label\n","test_labels = test_data[:, -1]  # The last column is the label\n","\n","# Normalize test features using mean and std from the training features\n","normalized_test_features = (test_features - mean) / std\n","\n","# Prediction function adapted for the trained SVM models\n","def predict(X, weights, biases):\n","    # Calculate the score for each classifier\n","    scores = np.array([X.dot(w) + b for w, b in zip(weights, biases)])\n","    # Determine the predicted class with the highest score for each sample\n","    predicted_classes = np.argmax(scores, axis=0)\n","    return predicted_classes\n","\n","# Predict labels for the normalized test features using the new model\n","predicted_labels = predict(normalized_test_features, optimal_weights, optimal_biases)\n","\n","# Calculate accuracy by comparing predicted labels with actual test labels\n","accuracy = np.mean(predicted_labels == test_labels)\n","print(f\"Accuracy on the test data: {accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"ECBeEKfmXt_u"},"source":["**Polynomial kernel**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0AJ7hmjZwuo2"},"outputs":[],"source":["def polynomial_kernel(X, Y=None, degree=3, gamma=None, coef0=1):\n","    if Y is None:\n","        Y = X\n","    if gamma is None:\n","        gamma = 1.0 / X.shape[1]  # Default value of gamma is 1 / n_features\n","\n","    K = (gamma * np.dot(X, Y.T) + coef0) ** degree\n","    return K"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2175,"status":"ok","timestamp":1711557525543,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"F_qSmaJlwczu","outputId":"08a0f9ac-2b07-4047-d153-9e2d448213df"},"outputs":[{"name":"stdout","output_type":"stream","text":["===============================================================================\n","                                     CVXPY                                     \n","                                     v1.3.3                                    \n","===============================================================================\n","(CVXPY) Mar 27 04:38:44 PM: Your problem has 100 variables, 3 constraints, and 0 parameters.\n","(CVXPY) Mar 27 04:38:44 PM: It is compliant with the following grammars: DCP, DQCP\n","(CVXPY) Mar 27 04:38:44 PM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n","(CVXPY) Mar 27 04:38:44 PM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n","-------------------------------------------------------------------------------\n","                                  Compilation                                  \n","-------------------------------------------------------------------------------\n","(CVXPY) Mar 27 04:38:44 PM: Compiling problem (target solver=SCS).\n","(CVXPY) Mar 27 04:38:44 PM: Reduction chain: FlipObjective -> Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> SCS\n","(CVXPY) Mar 27 04:38:44 PM: Applying reduction FlipObjective\n","(CVXPY) Mar 27 04:38:44 PM: Applying reduction Dcp2Cone\n","(CVXPY) Mar 27 04:38:44 PM: Applying reduction CvxAttr2Constr\n","(CVXPY) Mar 27 04:38:44 PM: Applying reduction ConeMatrixStuffing\n","(CVXPY) Mar 27 04:38:44 PM: Applying reduction SCS\n","(CVXPY) Mar 27 04:38:44 PM: Finished problem compilation (took 1.044e-01 seconds).\n","-------------------------------------------------------------------------------\n","                                Numerical solver                               \n","-------------------------------------------------------------------------------\n","(CVXPY) Mar 27 04:38:44 PM: Invoking solver SCS  to obtain a solution.\n","------------------------------------------------------------------\n","\t       SCS v3.2.4 - Splitting Conic Solver\n","\t(c) Brendan O'Donoghue, Stanford University, 2012\n","------------------------------------------------------------------\n","problem:  variables n: 100, constraints m: 201\n","cones: \t  z: primal zero / dual free vars: 1\n","\t  l: linear vars: 200\n","settings: eps_abs: 1.0e-05, eps_rel: 1.0e-05, eps_infeas: 1.0e-07\n","\t  alpha: 1.50, scale: 1.00e-01, adaptive_scale: 1\n","\t  max_iters: 100000, normalize: 1, rho_x: 1.00e-06\n","\t  acceleration_lookback: 10, acceleration_interval: 10\n","lin-sys:  sparse-direct-amd-qdldl\n","\t  nnz(A): 300, nnz(P): 5050\n","------------------------------------------------------------------\n"," iter | pri res | dua res |   gap   |   obj   |  scale  | time (s)\n","------------------------------------------------------------------\n","     0| 3.64e+00  2.25e+00  1.42e+01 -2.42e+01  1.00e-01  2.28e-02 \n","   125| 1.96e-06  2.55e-06  2.01e-05 -1.05e+01  3.27e-01  3.80e-02 \n","------------------------------------------------------------------\n","status:  solved\n","timings: total: 3.80e-02s = setup: 2.13e-02s + solve: 1.68e-02s\n","\t lin-sys: 3.54e-03s, cones: 1.27e-04s, accel: 5.07e-05s\n","------------------------------------------------------------------\n","objective = -10.485034\n","------------------------------------------------------------------\n","-------------------------------------------------------------------------------\n","                                    Summary                                    \n","-------------------------------------------------------------------------------\n","(CVXPY) Mar 27 04:38:44 PM: Problem status: optimal\n","(CVXPY) Mar 27 04:38:44 PM: Optimal value: 1.049e+01\n","(CVXPY) Mar 27 04:38:44 PM: Compilation took 1.044e-01 seconds\n","(CVXPY) Mar 27 04:38:44 PM: Solver (including time spent in interface) took 5.195e-02 seconds\n","Optimization successful.\n"]}],"source":["import numpy as np\n","import cvxpy as cp\n","\n","# Assuming normalized_features and labels are defined\n","# Use a small subset for debugging to prevent crashing\n","X_debug = normalized_features[:100]\n","y_debug = np.where(labels[:100] == 0, 1, -1)  # Example: Class 0 vs rest\n","\n","# Define polynomial kernel function\n","def polynomial_kernel(X, Y=None, degree=3, gamma=None, coef0=1):\n","    if Y is None:\n","        Y = X\n","    if gamma is None:\n","        gamma = 1.0 / X.shape[1]\n","    K = (gamma * np.dot(X, Y.T) + coef0) ** degree\n","    return K\n","\n","# Compute the kernel matrix for the debug subset\n","K_debug = polynomial_kernel(X_debug, degree=2, gamma=None, coef0=1)  # Using a simpler kernel for debugging\n","\n","# Symmetrize the kernel matrix used in quadratic form\n","Q_debug = np.multiply(np.outer(y_debug, y_debug), K_debug)\n","Q_sym_debug = 0.5 * (Q_debug + Q_debug.T)  # Symmetrizing the matrix\n","\n","n_samples_debug = X_debug.shape[0]\n","alpha_debug = cp.Variable(n_samples_debug)\n","\n","# Dual problem objective\n","objective_debug = cp.Maximize(\n","    cp.sum(alpha_debug) - 0.5 * cp.quad_form(alpha_debug, Q_sym_debug)\n",")\n","\n","# Constraints\n","constraints_debug = [\n","    alpha_debug >= 0,\n","    alpha_debug <= 1,  # Assuming C=1 for debugging\n","    cp.sum(cp.multiply(alpha_debug, y_debug)) == 0\n","]\n","\n","# Solve the problem for debugging\n","prob_debug = cp.Problem(objective_debug, constraints_debug)\n","result_debug = prob_debug.solve(solver=cp.SCS, verbose=True)\n","\n","# Check if the optimization was successful\n","if result_debug is not None:\n","    print(\"Optimization successful.\")\n","    optimized_alphas_debug = alpha_debug.value\n","    # Additional steps to calculate support vectors, weights, and bias would go here\n","else:\n","    print(\"Optimization failed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":363,"status":"ok","timestamp":1711557676416,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"rwj410SYyGA8","outputId":"e9ee9e7e-8daa-4282-a842-0e9569b61733"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of support vectors: 40\n"]}],"source":["# Threshold for considering an alpha to correspond to a support vector\n","alpha_threshold = 1e-4\n","\n","# Identifying the support vectors\n","support_vector_indices = np.where(optimized_alphas_debug > alpha_threshold)[0]\n","support_vectors = X_debug[support_vector_indices]\n","support_alphas = optimized_alphas_debug[support_vector_indices]\n","support_labels = y_debug[support_vector_indices]\n","\n","print(f\"Number of support vectors: {len(support_vector_indices)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":719,"status":"ok","timestamp":1711557845553,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"hUcRp7yjyZV3","outputId":"225d9a6b-e86f-4c80-e241-5cf82a32f8fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Bias (b): -0.5981252247196116\n"]}],"source":["import numpy as np\n","\n","# Define the polynomial kernel parameters\n","degree = 3\n","coef0 = 1\n","n_features = X_debug.shape[1]\n","gamma = 1.0 / n_features  # Define gamma as 1/n_features if not defined\n","\n","# Identifying the support vectors and their properties\n","alpha_threshold = 1e-4  # Threshold for alpha to identify support vectors\n","support_vector_indices = np.where(optimized_alphas_debug > alpha_threshold)[0]\n","support_vectors = X_debug[support_vector_indices]\n","support_alphas = optimized_alphas_debug[support_vector_indices]\n","support_labels = y_debug[support_vector_indices]\n","\n","# Compute the kernel between support vectors and all vectors\n","K_support_all = polynomial_kernel(support_vectors, X_debug, degree=degree, gamma=gamma, coef0=coef0)\n","\n","# Compute the bias term (b)\n","# Simplified approach: average over all support vectors\n","sum_b = 0\n","for i in range(len(support_vectors)):\n","    sum_b += support_labels[i] - np.sum(support_alphas * support_labels * K_support_all[:, i])\n","b = sum_b / len(support_vectors)\n","\n","print(f\"Bias (b): {b}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUBAFvOkz3HG"},"outputs":[],"source":["\n","# Extract test features and labels\n","test_features = test_data[:, :-1]  # Exclude the last column, which is the label\n","test_labels = test_data[:, -1]  # The last column is the label\n","\n","# Normalize test features using the mean and std from the training features\n","normalized_test_features = (test_features - mean) / std"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":432,"status":"ok","timestamp":1711558075374,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"nReTZJgdz5X-","outputId":"ca9e428d-2dad-4fef-979f-b00b763ecca6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on the test data: 0.1065\n"]}],"source":["def predict_with_kernel(X):\n","    # Compute the polynomial kernel between X and the support vectors\n","    K_test_support = polynomial_kernel(X, support_vectors, degree=degree, gamma=gamma, coef0=coef0)\n","\n","    # Calculate the decision function for each sample in X\n","    decision_values = np.dot(K_test_support, (support_alphas * support_labels)) + b\n","\n","    # Assign class labels based on the sign of the decision function\n","    return np.sign(decision_values)\n","\n","# Now that 'normalized_test_features' is defined, predict labels for the test set\n","predicted_labels = predict_with_kernel(normalized_test_features)\n","\n","# Convert predicted labels from 1, -1 to the original class labels for binary classification\n","predicted_labels_converted = np.where(predicted_labels == 1, 0, 1)  # Assuming class 0 vs rest, adjust as necessary\n","\n","# Calculate accuracy by comparing predicted labels with actual test labels\n","accuracy = np.mean(predicted_labels_converted == test_labels)\n","print(f\"Accuracy on the test data: {accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"iHi47_l-0e2s"},"source":["**Radial Basis Function (RBF) Kernel**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5279,"status":"ok","timestamp":1711558665543,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"fumOwZGxz8FD","outputId":"a37fd386-8e65-439f-9b47-5769c195d755"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimization successful.\n"]}],"source":["import numpy as np\n","from scipy.optimize import minimize\n","from numpy.linalg import norm\n","\n","subset_size = 100  # Adjust based on computational resources\n","np.random.seed(42)  # For reproducibility\n","indices = np.random.choice(len(normalized_features), size=subset_size, replace=False)\n","\n","X_subset = normalized_features[indices]\n","y_subset = np.where(labels[indices] == class_label, 1, -1)\n","\n","# Define the RBF kernel function\n","def rbf_kernel(x1, x2, gamma=1.0):\n","    dist_sq = norm(x1[:, None] - x2, axis=2) ** 2\n","    return np.exp(-gamma * dist_sq)\n","\n","# Gamma for the RBF kernel\n","gamma = 0.1\n","\n","# Regularization parameter C\n","C = 1.0  # Adjust based on your dataset\n","\n","# Dual problem objective function to be minimized\n","def dual_objective(alpha):\n","    K = rbf_kernel(X_subset, X_subset, gamma=gamma)\n","    half_quad_form = 0.5 * np.dot(alpha, np.dot(alpha, K * y_subset[:, None] * y_subset[None, :]))\n","    return half_quad_form - np.sum(alpha)\n","\n","# Constraint ensuring that the sum of alpha_i * y_i = 0\n","cons = ({'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y_subset)})\n","\n","# Bounds for alpha_i, 0 <= alpha_i <= C for all i\n","bounds = [(0, C) for _ in range(subset_size)]\n","\n","# Initial guess for alpha\n","initial_alpha = np.zeros(subset_size)\n","\n","# Solve the quadratic optimization problem\n","result = minimize(dual_objective, initial_alpha, method='SLSQP', bounds=bounds, constraints=cons)\n","\n","if result.success:\n","    alphas = result.x\n","    sv = alphas > 1e-5  # Threshold for determining support vectors\n","    support_vectors = X_subset[sv]\n","    support_labels = y_subset[sv]\n","    support_alphas = alphas[sv]\n","    # Calculate the bias term\n","    K_sv = rbf_kernel(support_vectors, support_vectors, gamma)\n","    b = np.mean([y_k - np.sum(support_alphas * support_labels * K_sv[i]) for i, y_k in enumerate(support_labels)])\n","    print(\"Optimization successful.\")\n","else:\n","    print(\"Optimization was not successful. Message:\", result.message)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1711558704641,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"zYQjiuUP2SMf","outputId":"94982c8c-2118-4dbe-aa91-16dac5543b41"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimized alpha values for support vectors:\n"," [0.24778312 0.05745584 0.31171832 1.         1.         0.1332226\n"," 0.08374385 1.         0.09487018 0.17078726 1.         0.17048859\n"," 0.13296417 0.12819877 1.         0.25003186 0.16971832 0.08200471\n"," 0.16354856 0.28470993 0.12634862 0.19433413 0.13222096 0.147097\n"," 0.16172827 0.16115765 0.13933604 0.05554849 0.13756415 0.14606729\n"," 1.         0.20516658 0.17581577 0.24459862 0.12832111 0.16080271\n"," 0.19880502 0.16646328 0.13503006 0.14590697 0.17372692 0.12522789\n"," 0.14908998 0.12285731 0.1579307  0.1478242  0.10099618 0.208695\n"," 0.05883275 0.13622678 0.16639841 0.12196853 0.20160277 0.14035349\n"," 1.         0.16780417 0.12524661 0.14651123 0.06847318 0.10788035\n"," 0.18469475 0.19747455 0.11395699 0.28488852 0.16264671 0.16306459\n"," 0.08076093 0.04688382 0.22925615 0.10118664 0.16742957 1.\n"," 0.1385258  0.05775931 0.13634216 1.         0.22700654 0.18094819\n"," 0.18115859 0.15297201 0.15526631 1.         1.         0.11352388\n"," 0.14505836 0.14018757 0.12897569 0.14496781 1.         0.1468089\n"," 1.         0.18383167 0.16759947 0.28045747 0.20521505 0.07904\n"," 0.23090677]\n","\n","Support vectors:\n"," [[ 0.11025858  0.70828057  0.14937323 ... -0.72135144  2.94039989\n","   0.79826196]\n"," [ 0.35504832 -0.67872934 -0.04783809 ... -1.21608981 -0.39559473\n","   0.81129524]\n"," [ 1.04896012  0.30777788 -0.55872976 ...  0.71275044 -1.02364661\n","  -1.59925815]\n"," ...\n"," [ 1.51812839 -1.43235876  0.0810933  ... -0.13596135  0.95153399\n","  -0.23328982]\n"," [ 0.6135776  -0.34947838  0.29931273 ... -1.03507054  1.06665638\n","  -0.41969488]\n"," [-0.41287729  0.18787152  0.35451837 ...  0.17027156 -0.23244579\n","  -1.00046758]]\n","\n","Labels for support vectors:\n"," [-1 -1 -1  1  1 -1 -1  1 -1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n"," -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n"," -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1\n"," -1 -1 -1  1 -1 -1 -1 -1 -1  1  1 -1 -1 -1 -1 -1  1 -1  1 -1 -1 -1 -1 -1\n"," -1]\n","\n","Bias term (b): -0.7024143597244928\n"]}],"source":["# Print the optimized parameters of the SVM\n","print(\"Optimized alpha values for support vectors:\\n\", support_alphas)\n","\n","# The support vectors themselves\n","print(\"\\nSupport vectors:\\n\", support_vectors)\n","\n","# The labels for the support vectors\n","print(\"\\nLabels for support vectors:\\n\", support_labels)\n","\n","# Bias term\n","print(\"\\nBias term (b):\", b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81j94qxq29Hd"},"outputs":[],"source":["\n","# Extract test features and labels\n","test_features = test_data[:, :-1]  # Exclude the last column, which is the label\n","test_labels = test_data[:, -1]  # The last column is the label\n","\n","# Normalize test features using the mean and std from the training features\n","normalized_test_features = (test_features - mean) / std"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3433,"status":"ok","timestamp":1711558893397,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"},"user_tz":-330},"id":"CUjy549M2izU","outputId":"af84a90f-bdd3-4e16-8983-f4e6c5949e00"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on the test data: 0.0984\n"]}],"source":["def predict_with_kernel(X, support_vectors, support_labels, support_alphas, b, gamma):\n","    # Compute the RBF kernel between X and the support vectors\n","    K_test_support = np.array([rbf_kernel(x[np.newaxis, :], support_vectors, gamma=gamma) for x in X])\n","\n","    # Calculate the decision function for each sample in X\n","    decision_values = np.dot(K_test_support, (support_alphas * support_labels)) + b\n","\n","    # Assign class labels based on the sign of the decision function\n","    return np.sign(decision_values)\n","\n","# Predict labels for the test set\n","predicted_labels = predict_with_kernel(normalized_test_features, support_vectors, support_labels, support_alphas, b, gamma)\n","\n","predicted_labels_converted = np.where(predicted_labels == 1, class_label, 0)  # Assuming class 0 vs rest as an example\n","\n","# Calculate accuracy by comparing predicted labels with actual test labels\n","accuracy = np.mean(predicted_labels_converted == test_labels)\n","print(f\"Accuracy on the test data: {accuracy:.4f}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNlTrj5TElsa9mdvKFMgxvD","mount_file_id":"1OwxxBeGYYTUGBbEAdrpfXnm7egZ-FwtQ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
